{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profanity Classifier Using Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will construct a profanity (insult) classifier using Naive Bayes, then using a combination of Logistic Regression and Naive Bayes. We will extensively use scikit learn, python's most popular Machine Learning Library, scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use adataset from the kaggle competition [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge). Kaggle is a website that hosts Machine Learning competitions and very good source of datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfInit = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfInit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Toxic Comment dataset contains several types of toxic comments; we are only interested in insults. Notice that the dataset is quite unbalanced. Indeed we will compute the proportion of insults in the comments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the proportion of data containing insults. We need to compute the [sum](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sum.html) of the __insult__ column, since it only contains 0 and 1, will give the number of rown with insults. Then divide by the number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "<FILL IN> /len(dfInit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the algorithm to properly learn to classify if a comment is an insult or not, we need a balanced dataset. We will first divide the dataframe into one with insults on the __insult__ column, and one with no insults. We will take a sample from the non-insult one, and re-merge them together so will will get a 50% insults dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfInsult = dfInit[dfInit['insult'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noInsults = len(dfInsult)\n",
    "noInsults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarily, construct a __dfNoInsult__ containing non insults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfNoInsult = <FILL IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfNoInsult = dfNoInsult.iloc[:noInsults,:] #only keep the same number of rows that exista in the insult dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [pandas concat](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html) to re-merge dfInsult and dfNoInsult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.concat(<FILL IN>).reset_index() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will select only the _comment_text_ and _insult_ column, we don't need information on the other classes for our exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data[['comment_text','insult']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "data = shuffle(data,random_state=23).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Features for text classification. Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train a model, we need features describing the comment. One way to do this is the so called __Bag of Words__ approach, where we consider the set of words within a document, without taking into accont their order.\n",
    "Let's take the following sencentes, that we will call documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = ['roses are red','violets are blue','this is your homework','some cats are red some cats are blue']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we construct the vocabulary, the set of all the words that appear in the documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = list(set((' '.join(sentences)).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will encode each document with a vector of length equals the lenth of the vocabulary. On each position, the value is the frequency of the term within the document. This model is called __term frequency__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this to sentence 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you need to count the occurances of an element in a list. For example,\n",
    "```python\n",
    "[1, 2, 3, 4, 1, 4, 1].count(1)\n",
    "```\n",
    "will output 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoding = []\n",
    "for word in vocab:\n",
    "    encoding.append(sentences[3].<FILL IN>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have encoded a document with a vector. We are also able to compute distance between vector encoded documents to find if documet B or document C is closer to document A. The most usual distance to be used for text is the cosine similarity, the cosine of the angle between vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this automatically with __CountVectorizer__ fromn scikit learn. The usual way to use a scikit-learn class has multiple stages: first we fit a model, and this corresponds to the training of an algorithm, where it apllies. Next, we predict a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each model, we begin by creating an instance on the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf1 = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we fit the model to the trainingset. Here, we fit it to _sentences_. Notice that this is more an explicite computation, then proper trainable algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf1.fit(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we transform the sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformed1 = tf1.transform(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real problems, the vocabulary size is quite big. If size of vocab is 10,000 words, and a sentence has 5 words, the resulting encoding will have thousands of null values that are worthless to store in memory. Therefore, the encoding is a [sparse vector](https://docs.scipy.org/doc/scipy/reference/sparse.html) only containing the values and positions of the non-null values. Since our example is based on a small vocabulary, we will transform the vectors to dense (i.e. non-sparse), to be able to visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transformed1.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the last row corresponds to the last document that we have manually encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term frequency-inverse document frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can enhance the TF approach by increasing the value for rare vectors, stating thatsentences that are similar in rare words are more similar then sentences similar in frequent words like 'and', 'the', etc\n",
    "The resulting model is called __Term Frequency-Inverse Document Frequency__, shortly TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will divide the already computed __TF__ with and __IDF__ vector; the i-th component of the IDF vector is the number of documents containing the i-th word in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docFrec = []\n",
    "for word in vocab:\n",
    "    docsWithWord = sum([word in sentence for sentence in sentences])\n",
    "    docFrec.append(docsWithWord / len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docFrec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(np.array(encoding) / np.array(docFrec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sikit-Learn has a TF-IDF transformer, namely TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula that is used to compute the $tf-idf$ of term $t$ is a little bit more complicated, but it measures thesame concept of documents being more similar because of common rare words: $tf-idf(d, t) = tf(t) * idf(d, t)$, and the $idf$ is computed as $idf(d, t) = \\log( \\frac{n}{df(d, t)}) + 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf2 = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that fit and transform can be performed together, using the [fit-transform](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformed2 =<FILL IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transformed2.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create features, train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply the above methods to create different sets of features. Let's first use CoutVectorizer to create TF features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf = CountVectorizer(strip_accents='ascii',max_df=0.8,min_df=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit-Transform the __comment text__ column of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "Xtf = tf.fit_transform(<FILL IN>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want a TF-IDF set of features. Moreover, we will not only use words, but also bigrams. A n-gram consists by n aconsecutive words. If we wound like to use fords and bigrams to encode the sentence \"I have a cat\", the corresponding vocab will be: I, have, a, cat, I have, have a, a cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,2), \n",
    "               min_df=3, max_df=0.9, strip_accents='ascii', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit-Transform the same column but with tfidf to create tfidf features with uni- and bi-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtfidf = <FILL IN>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y will be a numpy array with 0 or 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = data['insult'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also the train-test split can be easily performed with scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Naive Bayes with tf features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform Train-Test split with 67% of the data for training and 33% testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtf_train, Xtf_test, ytf_train, ytf_test = train_test_split(Xtf, y, \n",
    "                                                            test_size=0.33, random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $y$ denotes the class and $x_1,...,x_n$ denote the features, via Bayes Theorem, we have\n",
    "$$\n",
    "P(y\\mid x_1,...,x_n) = \\frac{P(y)P(x_1,...,x_n \\mid y)}{P(x_1,...x_n)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the naive independence hypothesis, \n",
    "$$\n",
    "P(y\\mid x_1,...,x_n) = \\frac{P(y)\\prod P(x_i \\mid y)}{P(x_1,...x_n)}\n",
    "$$\n",
    "that we want to maximize. Since the denominator is constant, it's enough to maximize the numerator, so Naive |Bayes will predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat y =\\underset{y}{\\mathrm{argmax}} P(y)\\prod P(x_i \\mid y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since TF numbers are intigers, we will use a multinomial Naive Bayes. Again, notice the order of the steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: construct an instance of the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnb.fit(Xtf_train,ytf_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ytf_pred_train = mnb.predict(Xtf_train)\n",
    "ytf_pred = mnb.predict(Xtf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Evaluate\n",
    "We will use accuracy, namely the proportion of good predictions. \n",
    "We will compute the accuracy on both test and training set, to find if our model si overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "(ytf_pred_train == ytf_train).sum() / len(ytf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(ytf_pred == ytf_test).sum() / len(ytf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "80% accuracy is pretty good and training and test accuracy are similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Naive Bayes with tfidf features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same with TF-IDF features and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtfidf_train, Xtfidf_test, ytfidf_train, ytfidf_test = train_test_split(Xtfidf, y, \n",
    "                                                            test_size=0.33, random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we will use Gaussian Naive Bayes, since tf-idf values are real numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will repeat the above steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gnb.fit(Xtfidf_train.toarray(),ytfidf_train) #dense is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ytfidf_pred_train = gnb.predict(Xtfidf_train.toarray())\n",
    "ytfidf_pred = gnb.predict(Xtfidf_test.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as we have done before, compute the train and test accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "<FILL IN> / len(ytfidf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "<FILL IN> / len(ytfidf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training accuracy has increased, but it overfits. I would prefer the first one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Naive Bayes and Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach is inspired by the article [Baselines and Bigrams: Simple, Good Sentiment and Topic Classification\n",
    "](https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf)\n",
    "We will compute naive bayes features and use logistic regression on top of it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the decision function for naive Bayes can be rewritten as \"predict class $0$ if the odds of $(0 \\mid \\mathbf {x} )$ exceed those of $(1\\mid \\mathbf {x} )$\". \n",
    "Expressing this in log-space gives:\n",
    "\n",
    "$${\\displaystyle \\log {\\frac {p(0\\mid \\mathbf {x} )}{p(1 \\mid \\mathbf {x} )}}=\\log p(0\\mid \\mathbf {x} )-\\log p(1 \\mid \\mathbf {x} )>0}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will multiply $X$ withthe ratio above, called log count ratio, we will get a Naive Bayes adjusted TF-IDF Features.\n",
    "Intuitivey, initially, we had the TF features. Then, we multiplied them with the idf to put a larger weight to rare words. After that, we multiply these with the above ratio, that increase further the weights that matter most for the naive bayes classifier to make decisions.\n",
    "We will use a Logistic Regression classifier on these NB features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compute count vectors _p1 = sum of all feature vectors with label 1_ and _p0 = sum of all feature vectors with label 0_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pr(y_i, y):\n",
    "    '''\n",
    "    function to compute the count vectors\n",
    "    ARGUMENTS: y_i, int, 0 or 1 and y, np.array\n",
    "    RERURNS: float\n",
    "    '''\n",
    "    p = x[y==y_i].sum(0)\n",
    "    return (p+1) / ((y==y_i).sum()+1) #add one to avoid having 0 at the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Xtfidf_train\n",
    "r = np.log(pr(1,ytfidf_train) / pr(0,ytfidf_train)) # log count ratio\n",
    "x_nb = x.multiply(r) #new X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "nblog = LogisticRegression(C=4, dual=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the [Logistic regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model \n",
    "using the above instance and the datasets __x_nb__ and __ytfidf_train__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_nbtfidf_pred_train = <FILL IN>\n",
    "(y_nbtfidf_pred_train == ytfidf_train).sum() / len(ytfidf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_nbtfidf_pred = nblog.predict(Xtfidf_test.multiply(r))\n",
    "(y_nbtfidf_pred == ytfidf_test).sum() / len(ytfidf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratz! You got 0.92 test accuracy, that is comparable to the most complex models in text classification!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's introduce more evaluation metrics, beside Accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some issues in using accuracy to assess classification. Let's say we are building a model for automatic interpretaion of an HIV test. Since the prelevance of HIV in Europe is less than 1%, the following funciton:\n",
    "\n",
    "```python\n",
    "def test(features):\n",
    "    \"\"\"\n",
    "    ARGUMENTS features, set of feaetures encoding microscopic sample\n",
    "    RETURNS 0 for HIV negative, 1 for HIV positive\n",
    "    \"\"\"\n",
    "    return 0\n",
    "```\n",
    "\n",
    "is a 99% accuracy automatic HIV tester. The __f1-score__ repares this. It is closesly related to two concepts: precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we are building an owl detection algorithm that identifies, in the following picture, the four red encircled owls:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/cat-and-owls.jpg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3 out of 4 identified owls are indeed owls; we say that our allgorithm's precision is 3/4\n",
    "* our algorithm has identified 3 out of 5 owls; we say that it has a recall of 4/5\n",
    "The average of the two measures would be a better candidate then the accuracy. Since both precision and recall are ratios, the harmonic average is a better case then the arithmetic average\n",
    "\n",
    "$$f_1 =2 \\frac{p \\cdot r}{p+r}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the f1 scores of the three models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('the f1 score using Multinomial Naive Bayes with TF features is %0.3f' % f1_score(ytf_pred,ytf_test))\n",
    "print('the f1 score using Gaussian Naive Bayes with TFIDF features is %0.3f' % f1_score(ytfidf_pred,ytfidf_test))\n",
    "print('the f1 score using Logistic Regression with Naive Bayes features is %0.3f' % f1_score(y_nbtfidf_pred,ytfidf_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more metric used in evaluating a classification algorithm is the Receiver Operating Characteristic (ROC-curve). Let's assume we take each example from test set that we have labeled with 1. If it is a 1, is a true posivite, if not, a false positive. At each moment, we can compute the true positive rate and false positive rate. \n",
    "A random classifier will give the red diagonal line, a perfect one will result in only a point in $(0,1)$. The greater the area (closer to 1) is, the better the classifier is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = roc_curve(ytfidf_test, y_nbtfidf_pred)\n",
    "roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Use the model for our own predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take an insult from [The french taunting from the movie Monty Python and the Holy Grail](https://www.youtube.com/watch?v=M9DCAFUerzs) and a noninsult and see how well the classifier performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test1 = 'Your mother was a hamster, and your father smelt of elderberries'\n",
    "test2 = 'roses are red, violets are blue, this was your homework, work done by you'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testlist = [test1, test2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_example = tfidf.transform(testlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nblog.predict(X_example.multiply(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to change the test1 and test2 with sentences of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the scikit-learn workflow:\n",
    "* class instance\n",
    "* fit\n",
    "* predict\n",
    "* evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
