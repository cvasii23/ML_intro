{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do the homework, you need to replace __<Fill In>__ with code that you will write. The cells where you have to enter code are prededed by markdown cells denoted with __TO DO:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Python lists, dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For people with no python background, [some tutorials](https://www.tutorialspoint.com/python/index.htm) on basic python, lists and dictionaries are recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy is the library that helps python dealing with matrices. Here are some very basic numpy tricks that we will use.\n",
    "\n",
    "We can transform a list of lists into a numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1,2,3],[-1,-2,-3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the transpose, we use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reshape numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B1 = A.reshape(1,6)\n",
    "B2 = A.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and stack (concaenate) them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.hstack([A,A])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may check a detailed [numpy tutorial](https://www.tutorialspoint.com/numpy/index.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the datas we need into two [pandas dataframes](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) called __df1__ and __df2__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data\\happiness2017.csv')\n",
    "df2 = pd.read_csv('data\\Alcohol_Consumption_Per_Country.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataframe is a tabular data type with an __index__ for row and __columns__ denoting column names. For example, running the next cell will provide the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't need all the data, we will drop some columns. It can be done with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.drop(['Happiness.Rank','Whisker.high','Whisker.low','Freedom','Generosity'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The name of the second column is impractically long, moreover, we prefer not to have blank spaces. Let's rename the colums of df2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns = ['Country','Lit_alcohol']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can [merge](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html) two dataframes on a column just like performing a join in SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TO DO:__\n",
    "\n",
    "Merge df1 and df2 on _Country_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(<Fill In>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('Country',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set the index on Country, filter only needed columns and rename them. Afterwards, we filter out the rown with __NA__ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.loc[:,['Health..Life.Expectancy.','Lit_alcohol']].rename(columns = {'Health..Life.Expectancy.' : 'expectancy', \n",
    "                                                                  'Lit_alcohol' : 'alcohol'})\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Machine Learning: Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to fit a regression model on the data above, to learn the relation between the $X=$ _alcohol consumption_ and $y=$ _life expectancy_, thus been able to answer questions about the ideal consumption of alcohol if we want to live a long life."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.jointplot(\"alcohol\", \"expectancy\", data=data, kind=\"reg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first try to fit a __linear model__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TO DO:__\n",
    "\n",
    "construct a function that returns $y =a x + b$ for given coefficiants $a,b$ and variable $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(a,b,x): \n",
    "    '''\n",
    "    function that computes y = a * x + b\n",
    "    ARGUMENTS: a,b,x floats\n",
    "    RETURNS: y, float\n",
    "    '''\n",
    "    return <FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of pairs $(x^{(i)}, y^{(i)})$, where $x^{(i)}$ denotes the average quantity of alcohol drinked in the country $i$, and $y^{(i)}$ denotes the life expectancy in the same country.\n",
    "For each value in the dataset, we will use the squared loss to measure the error between the actual $y^{(i)}$ and the predicted $\\hat y^{(i)}$: $\\mathcal L(y,\\hat y) = (y - \\hat y)^2$.We call _total loss_ the sum of losses for all the examples: $ \\mathcal L_{tot} = \\sum \\mathcal L(y^{(i)},\\hat y^{(i)})$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim is to minimize the cost $C = \\frac{1}{n} \\sum \\mathcal L(y^{(i)},\\hat y^{(i)})$. Note that minimzing the cost is the same with minimizing the total loss. We only divide with the number of examples because if we are training a model on a large number of examples, let's say  one million, than the cost will be quite large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TO DO:__\n",
    "\n",
    "Construnct three functions:\n",
    "* __err__ is an error function we will use to compare the actual value $y$ with the predicted $y_{pred}$ that is also denoted with $\\hat y$\n",
    "* __totalLoss__ that will compute the sum of the losses for each pair $(y^{(i)},\\hat y^{(i)})$. It actually\n",
    "* __costFunction__  Apply the previous __totalLoss__ to the model $y = a * x + b$ and divide by $n$, the number of datapoints\n",
    "* __mse__ is the square root of the cost fnction and stands for _Mean Squared Error_. It will be used to evaluate the model. Compare with the [standard deviation](https://en.wikipedia.org/wiki/Standard_deviation) notion from statistics.\n",
    "\n",
    "Hint: each of the functions uses the previous one.\n",
    "\n",
    "Hint for the total loss:\n",
    "\n",
    "```python\n",
    "np.array([1,2,3]).sum()\n",
    "```\n",
    "is valid in python, and gives 6. Therefore for the total loss function, there is no need to loop over examples to sum them up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def err(y_pred,y): \n",
    "    '''\n",
    "    function that computes the difference between Y_pred and y\n",
    "    ARGUMENTS: y,y_pred floats\n",
    "    RETURNS: y_pred - y, float\n",
    "    '''\n",
    "    return <FILL IN>\n",
    "\n",
    "def totalLoss(Y,Y_pred): \n",
    "    '''\n",
    "    function that calculates the total loss\n",
    "    ARGUMENTS: Y, Y_pred, array-like (numpy arrays, lists or series)\n",
    "    RETURNS: total loss, float\n",
    "    '''\n",
    "    return <FILL IN>\n",
    "\n",
    "def costFunction(y,a,b,x): \n",
    "    '''\n",
    "    function that computes the cost of the linear model y = a * x + b\n",
    "    ARGUMENTS: y, float, is the actual value; a,b are the learnable parameters of the model and x is the actual value\n",
    "    RETURNS: the cost, float\n",
    "    '''\n",
    "    return <FILL IN>\n",
    "\n",
    "def mse(y,a,b,x): \n",
    "    '''\n",
    "    Function that calculates the Mean Squared Error\n",
    "    ARGUMENTS: y, float, is the actual value; a,b are the learnable parameters of the model and x is the actual value\n",
    "    RETURNS: mse, float    \n",
    "    '''\n",
    "    return <FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider initial values for the parameters. Take X and y to be the desiered columns of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_guess=-1\n",
    "b_guess=10\n",
    "n = len(data)\n",
    "x = data['alcohol']\n",
    "y = data['expectancy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse(y, a_guess, b_guess, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint: the above mse = 6.1759400177221959"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We start with randomly chosen values of the parameters $a,b \\in R$ and update them via Gradient Descent, using the formula\n",
    "$$a =a -\\alpha \\frac{\\partial C}{\\partial a}$$\n",
    "$$b = b - \\alpha \\frac{\\partial C}{\\partial b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $\\alpha$, the learning rate, is a hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TO DO:__\n",
    "\n",
    "Implement a \n",
    "```python\n",
    "gradientDescent(X, y, theta, alpha, m, numIterations) \n",
    "```\n",
    "function. \n",
    "Here, $(X,y)$ denotes the input data. $X$ is the the feature matrix, the alcohol consumption and $y$, the life expectancy. Both are column matrices with each example $(x^{(i)},y^{(i)})$ on a row. For our problem, the _alcohol consumption_ is called feature, but it is also called independent variable or predictor. In some texts, $y$, the variable on which we want to make predictions, is called a dependent variable, because it depends on $X$. In our case, it is the _life expectancy_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, note that if we denote $\\theta = [b,a]^T$, then we can compute the hypothesis (also called model) $ax+b$ by matrix multiplication:\n",
    "$$ hypothesis = [1, x][b,a]^T $$\n",
    "You will implement the computation of the hypothesis, below, in this fashion. For matrix multiplication you will use [numpy dot](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.dot.html)\n",
    "\n",
    "Then the __sqLoss__ is a column matrix with the errors for each example. Note that python functions can be applied for arrays and it is considered pythonic to avoid for loops and other iterrations that can slow the program and also make it less readable. For example, if we use the err function that you have defined, first at scalar level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "err(3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the next cell to apply it at vector level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = np.array([3,4])\n",
    "B = np.array([2,6])\n",
    "err(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m denotes the number of examples here, not the number of features. \n",
    "#It is not computationally necessary but only kept for the formulae not to lose the connection with the mathematics.\n",
    "def gradientDescent(X, y, theta, alpha, m, numIterations):\n",
    "    '''\n",
    "    function that computes a given number of gradient descent iterations.\n",
    "    ARGUMENTS:\n",
    "    X, numpy array where the first column each element is 1 and the other columns are features\n",
    "    y, numpy array, column matrix with the variable we want to make predictions on\n",
    "    theta, numpy array cinsisting of [a,b]\n",
    "    alpha, float, the learning rate\n",
    "    m, int, the number of training examples\n",
    "    numIterations, int, the number of iterations\n",
    "    RETURNS: theta, numpy array with the updated values of a nd b\n",
    "    ''' \n",
    "    for i in range(numIterations):\n",
    "        hypothesis = np.dot(<Fill In>)\n",
    "        #loss = (hypothesis - y)**2. We now compute the sqare root of the loss.\n",
    "        sqLoss = err(<Fill In>).values\n",
    "        # avg cost per example (the 2 in 2*m doesn't matter and was only included to be consistent with the gradient\n",
    "        cost = np.sum(sqLoss ** 2) / (2 * m)\n",
    "        if i %100 == 0:\n",
    "            print(\"Iteration %d | Cost: %f\" % (i, cost))\n",
    "        # avg gradient per example\n",
    "        gradient = np.dot(X.T, sqLoss) / m\n",
    "        # update\n",
    "        theta = theta - alpha * gradient\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now able to apply a few iterations of gradient descend. For our example, $X$ is a column matrix with the alcohol consumption, and to be able to apply the above function, we need to concatenate a column of ones. \n",
    "\n",
    "__TO DO:__\n",
    "\n",
    "* use [np.ones](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.ones.html) to construnct a column matrix with all elements =1\n",
    "\n",
    "* use [np.hstack](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.hstack.html) to concatenate the two matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([b_guess,a_guess])\n",
    "X1 = <Fill In> # matrix of onex of len(x) rows and 1 column\n",
    "X2 = np.array(x).reshape(-1,1) #x reshaped\n",
    "X = <Fill In> #X1 and X2 stacked\n",
    "gradientDescent(X,y, theta, 0.01,len(x) ,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run now 1500 iterrations of gradient descent and also calculate the predicted $y_p$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp = np.arange(-5, 25)\n",
    "Xp = np.hstack([np.ones((len(xp),1)),np.array(xp).reshape(-1,1)])\n",
    "theta1 = gradientDescent(X,y, theta, 0.01,len(x) , 1500)\n",
    "yp = np.dot(Xp, theta1)\n",
    "print(theta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Checkpoint__ \n",
    "The last printed values must be\n",
    "\n",
    "Iteration 1400 | Cost: 0.020905\n",
    "\n",
    "[ 0.43847548  0.02551002]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to plot the resulted model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=100, figsize=(7, 7))\n",
    "x1,x2,y1,y2 = plt.axis()\n",
    "plt.axis((-5,25,-0.3,1.2))\n",
    "plt.scatter(x,y)\n",
    "plt.plot(xp,yp,color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(theta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Model\n",
    "\n",
    "If we take a look at the data, the regression line looks quite good. However, a parabola might better fit the data. Therefore, let's now look for a model $y=ax^2 + bx + c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can regard $x^2$ as another feature, so we want to estimate the life expenctancy depending on two features: $x_1=x$, the alcohol consumption and $x_2 = x^2$, the square of the alcohol consumption. Because of the vectorized way we have defined the gradient descent, the function also works in this case.\n",
    "\n",
    "The following stacks together a column of ones, $x$ and $x^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = np.hstack([np.ones((len(x),1)),np.array([x.tolist(),[el **2 for el in x.tolist() ]]).T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta2_init =  np.array([-0.33, 0.05 ,-0.006])# np.array([-1,23,-109])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run a few iterations of Gradient descent on this new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta2 = gradientDescent(X2,y, theta2_init , 0.0001,len(x) , 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(theta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xp2 = np.hstack([np.ones((len(xp),1)),np.array([xp.tolist(),[el **2 for el in xp.tolist() ]]).T])\n",
    "yp2 = np.dot(Xp2, theta2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can also plot this new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=100, figsize=(7, 7))\n",
    "x1,x2,y1,y2 = plt.axis()\n",
    "plt.axis((-5,25,-0.3,1.2))\n",
    "plt.scatter(x,y)\n",
    "plt.plot(xp,yp2,color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of these two is better?\n",
    "How about a fifth degree polynomial? We will repeat the previous steps to run a fifth degree model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "liste = []\n",
    "for i in range(1,6):\n",
    "    liste.append([el **i for el in x.tolist()])\n",
    "liste = [(np.array(A).T).reshape(-1,1) for A in liste]\n",
    "\n",
    "\n",
    "liste.insert(0,np.ones((len(x),1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(liste))\n",
    "X5 = np.hstack(liste)\n",
    "print(X5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta5_init =  np.array([  3.53953705e-01,   1.58855119e-01,  -3.27164561e-02,\n",
    "         1.63748493e-03,   1.18560697e-04,  -8.09401393e-06]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta5 = gradientDescent(X5,y, theta5_init , 0.00000000000001,len(x) , 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_p = []\n",
    "for i in range(1,6):\n",
    "    liste_p.append([el **i for el in xp.tolist()])\n",
    "liste_p = [(np.array(A).T).reshape(-1,1) for A in liste_p]\n",
    "liste_p.insert(0,np.ones((len(xp),1)))\n",
    "X5p = np.hstack(liste_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y5p = np.dot(X5p, theta5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(dpi=100, figsize=(7, 7))\n",
    "x1,x2,y1,y2 = plt.axis()\n",
    "plt.axis((-5,25,-0.3,1.2))\n",
    "plt.scatter(x,y)\n",
    "plt.plot(xp,y5p,color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "We need a way to examine and compare models to be able to choose the best. For this purpose, we will construct a few metrics.\n",
    "\n",
    "Remark: notice that the poly degree is also a hyperparameter, because it is not trainable and for each value we are having a new model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One such metric is the __ root mean squared error__ denoted RMSE. Formally it is the squared root of the cost, but coneptually, it is computer after the training of the model si done:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TO DO:__\n",
    "\n",
    "Implement a function to compute RMSE. You may use the formula:\n",
    "$$RMSE = \\frac{1}{2m}\\sqrt{\\sum(\\hat{y} -y)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RMSE(yhat,y):\n",
    "    '''\n",
    "    function that computes RMSE\n",
    "    ARGUMENTS: \n",
    "    yhat, numpy array, the prediction\n",
    "    y, numpy array, actual values\n",
    "    RETURNS:\n",
    "    RMSE, float\n",
    "    '''\n",
    "    m = len(y)\n",
    "    sqLoss = <Fill In>\n",
    "    cost = <Fill In>\n",
    "    return <Fill In>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE(np.dot(X, theta1),y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Checkpoint__\n",
    "\n",
    "The above RMSE should give 0.0086737885652513561"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE(np.dot(X2, theta2),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE(np.dot(X5, theta5),y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another metric is the $R-$ squared score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we denote by $\\bar y$ the mean value of y, then the $R^2$ is computed as:\n",
    "\n",
    "$R^2 = 1 - \\frac{\\sum(y_i - \\hat y_i)^2}{\\sum (y_i - \\bar y)^2}$\n",
    "\n",
    "Try to figure out an interpretation for the R squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def R2(yhat,y):\n",
    "    m = len(y)\n",
    "    bary = np.mean(y)\n",
    "    sqLoss1 = err(bary,y).values\n",
    "    sqLoss2 = err(yhat,y).values\n",
    "    cost1 = np.sum(sqLoss1 ** 2) \n",
    "    cost2 = np.sum(sqLoss2 ** 2) \n",
    "    return 1 - (cost2 / cost1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2(np.dot(X2, theta2),y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that the above considerations haven't got the relevance that we hoped for. In fact, the problem is that we have evaluated on the same dataset that the algorithm has learned from. In practice we are doing a train-test split, usually with 70% in the training set and 30% in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all = x.values\n",
    "y_all = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.permutation(x_all.shape[0])\n",
    "split = int(0.7 * x_all.shape[0])\n",
    "training_idx, test_idx = indices[:split], indices[split:]\n",
    "x_train, x_test = x[training_idx], x[test_idx]\n",
    "y_train, y_test = y[training_idx], y[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran 1-14 degree polinomial models (i.e. I ran these at home) and provided the data in a csv. Run the followind last two cells. What model would you choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfERR = pd.read_csv('data/erori.csv')\n",
    "dfERR.set_index('deg',inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfERR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfERR.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Based on the above, what model mould you choose? Why? Sent me the answers via e-mail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
